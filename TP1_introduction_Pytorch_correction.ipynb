{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbdca17-672d-4375-ad89-8a533a25b350",
   "metadata": {},
   "source": [
    "## Initiation aux modèles CNN avec la bibliothèque pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd3b73-0063-4f28-b0dd-e062305be9a1",
   "metadata": {},
   "source": [
    "Dans un premier temps nous allons construire une des premières architectures de CNN présenté par [Yann Le Cun](https://fr.wikipedia.org/wiki/Yann_Le_Cun), un [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).\n",
    "\n",
    "L'architecture du LeNet est rappelée dans la figure ci-dessous:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a5684-8aa0-46f1-a0f4-408854af6809",
   "metadata": {},
   "source": [
    "![leNet5.png](leNet5.jpeg \"Architecture Lenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245854e7-42be-4ec4-9d7d-63370fc3b18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c88a673f70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on importe les bibliothèques pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332ec33-92dc-45ab-9a7d-91754256cf57",
   "metadata": {},
   "source": [
    "### Définition de la dataset [MNIST](https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_MNIST) ainsi que ces transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460f3a0b-0108-4614-8acf-1c1ac0d02588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms # On peut inporter directement la dataset de pytorch\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "# On définit transforms qui permet de redimensionner l'image en 32*32 et de la transformer en tensor\n",
    "transforms = transforms.Compose([transforms.Resize((32, 32)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# On télécharge et on créer la dataset d'entraienement à l'aide du module datasets de torchvision\n",
    "train_dataset = datasets.MNIST(root='mnist_data', \n",
    "                               train=True, \n",
    "                               transform=transforms,\n",
    "                               download=True)\n",
    "\n",
    "# On télécharge et on créer la dataset de test à l'aide du module datasets de torchvision\n",
    "valid_dataset = datasets.MNIST(root='mnist_data', \n",
    "                               train=False, \n",
    "                               transform=transforms)\n",
    "\n",
    "BATCH_SIZE = 32 #taille du batch size\n",
    "\n",
    "# On définit le data loaders d'entraienement . Le data loaders permet de créer des batchs. On doit lui renseigner le batch size.\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "# On définit le data loaders de validation . \n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b41a7f-a59c-48fe-9ed6-e9d3d5e6e976",
   "metadata": {},
   "source": [
    "### Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641bf1fd-3f3a-4394-8fcf-760b1eeed9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (8): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (9): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LeNet(nn.Module): # On créer la classe LeNet qui hérite de la classe mère Module\n",
    "\n",
    "    def __init__(self): # On définit \n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(  \n",
    "            # Couche 1 (C1) : La première couche convolutive avec 6 noyaux de taille 5×5 et le stride de 1. \n",
    "            # Étant donné la taille de l'entrée (32×32×1), la sortie de cette couche est de taille 28×28×6.\n",
    "            # kernel\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Couche 2 (S2) : Une couche de sous-échantillonnage/mise en commun avec 6 noyaux de taille 2×2.\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            \n",
    "            \n",
    "            #Couche 3 (C3) : La deuxième couche convolutive avec la même configuration que la première, cependant, \n",
    "            #cette fois avec 16 filtres. La sortie de cette couche est de 10×10×16.\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Couche 4 (S4) : La deuxième couche de mise en commun. La logique est identique à celle de la précédente, \n",
    "            #mais cette fois, la couche comporte 16 filtres. La sortie de cette couche est de taille 5×5×16.\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            \n",
    "            #Couche 5 (C5) : La dernière couche convolutive avec 120 noyaux 5×5. \n",
    "            #Étant donné que l'entrée de cette couche est de taille 5×5×16 et que les noyaux sont de taille 5×5, \n",
    "            #la sortie est 1×1×120. Par conséquent, les couches S4 et C5 sont entièrement connectées. \n",
    "            #C'est aussi pourquoi dans certaines implémentations de LeNet-5, on utilise une couche entièrement connectée au lieu d'une couche convolutive comme 5ème couche. \n",
    "            #La raison pour laquelle cette couche reste une couche convolutive est le fait que si l'entrée du réseau est plus grande que celle utilisée dans l'entrée initiale (donc 32×32 dans ce cas), \n",
    "            #cette couche ne sera pas une couche entièrement connectée, car la sortie de chaque noyau ne sera pas 1×1.\n",
    "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        ## La dernière couche est un réseau de neurones simple\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=84, out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # on défini le passage de nos données\n",
    "        \n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs\n",
    "        \n",
    "\n",
    "net = LeNet()\n",
    "print(net) # On peut afficher les paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d7d9eda-d9ae-4858-ae9c-c5795fee7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On créer la fonction qui permet d'entrainer le modèle \n",
    "\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    '''\n",
    "    Function for the training step of the training loop\n",
    "    '''\n",
    "    model.train() # on passe le modèle en entrainement\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in train_loader:# on itére sur les données d'entrainement \n",
    "\n",
    "        optimizer.zero_grad() # on initialise l'erreur du gradient à zéro\n",
    "        \n",
    "        X = X.to(device) # on envoie les données X sur la GPU\n",
    "        y_true = y_true.to(device) # on envoie les données Y sur la GPU\n",
    "    \n",
    "        # Forward pass (on passe les données dans le modèle)\n",
    "        y_hat, _ = model(X) \n",
    "        loss = criterion(y_hat, y_true) # On calcul l'erreur du modèle avec la loss choisie\n",
    "        # Rétropropagation du gradient\n",
    "        loss.backward() \n",
    "        optimizer.step() # Descente de gradient (une itération)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    running_loss /= len(train_loader)\n",
    "    return model, optimizer, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd110d6-1864-47f3-a3e3-bf7d2013c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On créer la fonction qui permet de valider le modèle \n",
    "\n",
    "def validate(valid_loader, model, criterion):\n",
    "    '''\n",
    "    Function for the validation step of the training loop\n",
    "    '''\n",
    "   \n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in valid_loader:\n",
    "    \n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # Forward pass and record loss\n",
    "        y_hat, _ = model(X) \n",
    "        loss = criterion(y_hat, y_true) \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss /= len(valid_loader)\n",
    "    return model, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223babf4-394e-4a85-92c6-6f509f1d88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, print_every=1):\n",
    "    \n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        \n",
    "        # training\n",
    "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    \n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad(): # On désactive le gradient\n",
    "            model, valid_loss = validate(valid_loader, model, criterion)\n",
    "    \n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            print(\"epoch\",epoch)\n",
    "            print(\"Train loss\",train_loss)\n",
    "            print(\"Valid loss\",valid_loss)\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fd4c99-9969-4868-8adf-80230b71fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfea0f31-37e1-41b6-ae80-f2e110e024ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.16170943825278655\n",
      "Valid loss 0.028729197495604472\n",
      "Train loss 0.13781586179321514\n",
      "Valid loss 0.02227735337645937\n",
      "Train loss 0.129702695873864\n",
      "Valid loss 0.02859245139701868\n",
      "Train loss 0.12465118943516852\n",
      "Valid loss 0.025408306968363006\n",
      "Train loss 0.10804923721391094\n",
      "Valid loss 0.028857562214756917\n",
      "Train loss 0.09259195365094099\n",
      "Valid loss 0.025678622803849638\n",
      "Train loss 0.09941292782423987\n",
      "Valid loss 0.034739617918578776\n",
      "Train loss 0.08249669800973376\n",
      "Valid loss 0.03446525464860366\n",
      "Train loss 0.0811123874683862\n",
      "Valid loss 0.027356936942945352\n",
      "Train loss 0.07765725359103709\n",
      "Valid loss 0.024225109543208376\n",
      "Train loss 0.06980807149237736\n",
      "Valid loss 0.02707069431498533\n",
      "Train loss 0.06538203626365094\n",
      "Valid loss 0.024094998203282033\n",
      "Train loss 0.06122276970088275\n",
      "Valid loss 0.026316775353564935\n",
      "Train loss 0.06167250515005844\n",
      "Valid loss 0.027720548768944607\n",
      "Train loss 0.05706705474482516\n",
      "Valid loss 0.02953696291071564\n",
      "Train loss 0.06060498551991691\n",
      "Valid loss 0.025077295111573673\n",
      "Train loss 0.05122732035901493\n",
      "Valid loss 0.02746394160667713\n",
      "Train loss 0.05539708236922023\n",
      "Valid loss 0.024682251311400415\n",
      "Train loss 0.044104777070912315\n",
      "Valid loss 0.02781243086036771\n",
      "Train loss 0.0408774106603067\n",
      "Valid loss 0.030594977882127505\n",
      "Train loss 0.05266907671320266\n",
      "Valid loss 0.033130973198338\n",
      "Train loss 0.04236700253244403\n",
      "Valid loss 0.026553201259563484\n",
      "Train loss 0.04957192519848599\n",
      "Valid loss 0.02299173830803334\n",
      "Train loss 0.0342350961847651\n",
      "Valid loss 0.022655694546312367\n",
      "Train loss 0.04342728817998388\n",
      "Valid loss 0.026772259294647084\n",
      "Train loss 0.04726853187460906\n",
      "Valid loss 0.025954599038697082\n",
      "Train loss 0.03527032689165187\n",
      "Valid loss 0.02532516402834343\n",
      "Train loss 0.0313325528521281\n",
      "Valid loss 0.03244341046137504\n",
      "Train loss 0.03392787870593312\n",
      "Valid loss 0.03470456123740431\n",
      "Train loss 0.033293905323692266\n",
      "Valid loss 0.03132462614170918\n",
      "Train loss 0.03788035442330355\n",
      "Valid loss 0.03887237198284044\n",
      "Train loss 0.03344178750349803\n",
      "Valid loss 0.03352620744428961\n",
      "Train loss 0.03424636936322741\n",
      "Valid loss 0.02771681124642039\n",
      "Train loss 0.0354595393769397\n",
      "Valid loss 0.028786076290005947\n",
      "Train loss 0.032525530017713106\n",
      "Valid loss 0.03000295424941053\n",
      "Train loss 0.029052371262869192\n",
      "Valid loss 0.030520610915998597\n",
      "Train loss 0.030962040493425002\n",
      "Valid loss 0.03435264053601622\n",
      "Train loss 0.0243430879468616\n",
      "Valid loss 0.03127101817643586\n",
      "Train loss 0.03283068611972898\n",
      "Valid loss 0.03447646451360896\n",
      "Train loss 0.027263535327605337\n",
      "Valid loss 0.03682498168639483\n",
      "Train loss 0.03659722953952104\n",
      "Valid loss 0.03395838617349268\n",
      "Train loss 0.02394636961632824\n",
      "Valid loss 0.0343506500137468\n",
      "Train loss 0.026900399768548418\n",
      "Valid loss 0.03908086677518672\n",
      "Train loss 0.026262206616774874\n",
      "Valid loss 0.03971070117781798\n",
      "Train loss 0.02938086799781431\n",
      "Valid loss 0.042846046663162635\n",
      "Train loss 0.027112352454635075\n",
      "Valid loss 0.03265700258653387\n",
      "Train loss 0.018757906497407887\n",
      "Valid loss 0.0372013309748104\n",
      "Train loss 0.02852938726369852\n",
      "Valid loss 0.03673361154987721\n",
      "Train loss 0.02707575120880011\n",
      "Valid loss 0.03517287098955237\n",
      "Train loss 0.02842524044616453\n",
      "Valid loss 0.04075936387543751\n",
      "Train loss 0.02124256426747453\n",
      "Valid loss 0.03447018035224744\n",
      "Train loss 0.024140612112904376\n",
      "Valid loss 0.034757562838798654\n",
      "Train loss 0.02128021194407959\n",
      "Valid loss 0.0377465221863147\n",
      "Train loss 0.022347699891951935\n",
      "Valid loss 0.03863319971786141\n",
      "Train loss 0.02581952455865333\n",
      "Valid loss 0.031846136832854297\n",
      "Train loss 0.020870291186281052\n",
      "Valid loss 0.036368115980455674\n",
      "Train loss 0.022389557427540976\n",
      "Valid loss 0.03792574568790559\n",
      "Train loss 0.019064531226901577\n",
      "Valid loss 0.038265898830959606\n",
      "Train loss 0.027951162599146654\n",
      "Valid loss 0.039809872488071635\n",
      "Train loss 0.02029819144194733\n",
      "Valid loss 0.0396657362888301\n",
      "Train loss 0.022550715053064444\n",
      "Valid loss 0.03570451583392318\n",
      "Train loss 0.022865253079846146\n",
      "Valid loss 0.041952942031265414\n",
      "Train loss 0.019437170646465354\n",
      "Valid loss 0.033363729647620625\n",
      "Train loss 0.015141563735243321\n",
      "Valid loss 0.03480703003561366\n",
      "Train loss 0.02292505032034154\n",
      "Valid loss 0.040167950545558426\n",
      "Train loss 0.027243832397181304\n",
      "Valid loss 0.03855859844656261\n",
      "Train loss 0.02033317052112306\n",
      "Valid loss 0.034299614917609966\n",
      "Train loss 0.01935970986028678\n",
      "Valid loss 0.03464628286295706\n",
      "Train loss 0.026954289842669756\n",
      "Valid loss 0.03857023478704455\n",
      "Train loss 0.01780757116699646\n",
      "Valid loss 0.02950169623933145\n",
      "Train loss 0.023229300932413762\n",
      "Valid loss 0.03464781088378072\n",
      "Train loss 0.019863429831462\n",
      "Valid loss 0.037052188438327395\n",
      "Train loss 0.017398376020539022\n",
      "Valid loss 0.032800065711645024\n",
      "Train loss 0.01721484019134907\n",
      "Valid loss 0.037335152694772535\n",
      "Train loss 0.024686494409728907\n",
      "Valid loss 0.03469172974017538\n",
      "Train loss 0.014984372999033364\n",
      "Valid loss 0.03567303964763943\n",
      "Train loss 0.021997334439746635\n",
      "Valid loss 0.04103186457470017\n",
      "Train loss 0.021392795192445573\n",
      "Valid loss 0.036462541248699645\n",
      "Train loss 0.015633718959596296\n",
      "Valid loss 0.04491712806372245\n",
      "Train loss 0.014418300657633951\n",
      "Valid loss 0.04072971739696118\n",
      "Train loss 0.016116765842717633\n",
      "Valid loss 0.05265804204772258\n",
      "Train loss 0.01732647370749155\n",
      "Valid loss 0.039164376146287594\n",
      "Train loss 0.02224049690143724\n",
      "Valid loss 0.03920902858436916\n",
      "Train loss 0.014400110586344175\n",
      "Valid loss 0.04031623896108747\n",
      "Train loss 0.01608328064181061\n",
      "Valid loss 0.04105871669754839\n",
      "Train loss 0.02162195185440972\n",
      "Valid loss 0.0371002259203934\n",
      "Train loss 0.019569711680374014\n",
      "Valid loss 0.041064046423815\n",
      "Train loss 0.022238563396126995\n",
      "Valid loss 0.03944354127078171\n",
      "Train loss 0.012575632862222155\n",
      "Valid loss 0.032601976970191154\n",
      "Train loss 0.01998753334748734\n",
      "Valid loss 0.036335502683924416\n",
      "Train loss 0.019357735052830703\n",
      "Valid loss 0.03914425057926162\n",
      "Train loss 0.016877048078094755\n",
      "Valid loss 0.03327837472156982\n",
      "Train loss 0.016637747702745242\n",
      "Valid loss 0.04255377866391502\n",
      "Train loss 0.012019816240501418\n",
      "Valid loss 0.04180023429662724\n",
      "Train loss 0.014911775334970824\n",
      "Valid loss 0.05625595077895171\n",
      "Train loss 0.01943048105709222\n",
      "Valid loss 0.04331628150746269\n",
      "Train loss 0.008480738072358053\n",
      "Valid loss 0.04230940280957648\n",
      "Train loss 0.020456207781506787\n",
      "Valid loss 0.03771996079555398\n",
      "Train loss 0.013988463447545103\n",
      "Valid loss 0.043785766181596644\n",
      "Train loss 0.014989620690859202\n",
      "Valid loss 0.03833784077310437\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "model, optimizer = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f605b-a6b2-4e34-9339-c7e430183737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f3bff-470d-4409-9357-5f9f66e36180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
