{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364da5ad-1eb5-49c6-870b-960db1e83d80",
   "metadata": {},
   "source": [
    "___\n",
    "## <span style='color:#0d5874'> Travaux pratiques : introduction aux modèles génératifs </span>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60954a32-6413-47bf-8447-07a9c0df94c1",
   "metadata": {},
   "source": [
    "<span style='color:Red'>**L’objectif de cette séance de TP est de réaliser un bref tour d’horizon des modèles génératifs auto-encodeur et des génératifs**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d1049-fed1-4cfc-97a7-33083eeeeb95",
   "metadata": {},
   "source": [
    "---\n",
    "<span style='color:#0d5874'> **Exercice 1 : Autoencodeurs et génération d’images** </span>\n",
    "\n",
    "---\n",
    "Cet exercice présente l’utilisation des autoencodeurs avec PyTorch. Nous allons utiliser un modèle simple, entièrement connecté, permettant de compresser une image en une représentation vectorielle. Cet exercice utilise MNIST comme jeu de données vues précédemment dans le TP1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26081ce-97d3-472d-aa81-574b6cdb187d",
   "metadata": {},
   "source": [
    "![autoencoder.png](autoencoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f54bd2-202b-4a71-980c-c36bd1cf67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports liés à torch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861e440e-aa18-4fe5-8e4d-161bbaab7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms # On peut inporter directement la dataset de pytorch\n",
    "from torch.utils.data import DataLoader \n",
    "BATCH_SIZE = 128\n",
    "transform = transforms.Compose([transforms.ToTensor(),  transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# On télécharge et on créer la dataset d'entraienement à l'aide du module datasets de torchvision\n",
    "train_dataset = datasets.MNIST(root='mnist_data', \n",
    "                               train=True, \n",
    "                               transform=transform,\n",
    "                               download=True)\n",
    "\n",
    "# On télécharge et on créer la dataset de test à l'aide du module datasets de torchvision\n",
    "valid_dataset = datasets.MNIST(root='mnist_data', \n",
    "                               train=False, \n",
    "                               transform=transform)\n",
    "\n",
    "# On définit le data loaders d'entraienement . Le data loaders permet de créer des batchs. On doit lui renseigner le batch size.\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "# On définit le data loaders de validation . \n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0f356-2cc8-491e-8f6b-ac4ed20996ec",
   "metadata": {},
   "source": [
    "---\n",
    "<span style='color:Green'>**Question**</span>\n",
    "\n",
    "En utilisant l’interface [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) de PyTorch, écrire le code qui définit un modèle autoencodeur entièrement connecté.\n",
    "\n",
    "Ce modèle prend en entrée une image de dimensions 28×28 sous forme d’un vecteur aplati de longeur 784. On définira une variable latent_dimension qui permet de contrôler la taille du code z en sortie de l’encodeur.\n",
    "\n",
    "Le décodeur devra prendre en entrée un code z de longueur latent_dimension et produire un vecteur aplati de longueur 28×28=784 (identique à l’image). On choisira une valeur raisonnable pour la dimension du code (par exemple, entre 30 et 250).\n",
    "\n",
    "encoder : \n",
    "\n",
    "1. Linear(in_features=784, out_features=1024, bias=True) ReLU()\n",
    "2. Linear(in_features=1024, out_features=256, bias=True) ReLU()\n",
    "3. Linear(in_features=256, out_features=128, bias=True)\n",
    "\n",
    "decoder:\n",
    "\n",
    "1. Linear(in_features=128, out_features=256, bias=True) ReLU()\n",
    "2. Linear(in_features=256, out_features=1024, bias=True) ReLU()\n",
    "3. Linear(in_features=1024, out_features=784, bias=True) Sigmoid()\n",
    "\n",
    "---\n",
    "\n",
    "**Indice**\n",
    "Indice: l’utilisation de la méthode [.view()](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) ou de la couche [nn.Flatten()](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) peut être utile pour ré-arranger les tenseurs avant ou après les couches linéaires. Par exemple, x.view(-1, 1, 28, 28) permet de transformer un tenseur de dimensions 784 en un tenseur de dimensions (batch, 1, 28, 28)…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8121cb5f-1567-4de3-bc9b-9f5d43a4c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto_encoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=784, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Auto_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,latent_dimension): # On définit \n",
    "        super(Auto_encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "         ###### Votre code ici ########\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "        \n",
    "            ###### Votre code ici ########\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): # on défini le passage de nos données\n",
    "            x = torch.flatten(x,1)\n",
    "            z = self.encoder(x)\n",
    "            x_hat = self.decoder(z) \n",
    "            x_hat = x_hat.view(-1,1,28,28)\n",
    "            return x_hat\n",
    "    \n",
    "auto_encoder = Auto_encoder(latent_dimension = 128)\n",
    "print(auto_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad601f-6b56-45c0-a7dc-e6977529a348",
   "metadata": {},
   "source": [
    "---\n",
    "<span style='color:Green'>**Question**</span>\n",
    "\n",
    "---\n",
    "Initialisez la loss; Comme paramétre de criterion on prendra la L1Loss et l'optimizer Adam avec lr=0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6419676b-1f5c-487d-913c-901eea25b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(auto_encoder.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f658d48-6d83-40f6-88ce-8b93aa7c0772",
   "metadata": {},
   "source": [
    "---\n",
    "<span style='color:Green'>**Question**</span>\n",
    "\n",
    "---\n",
    "Créer une fonction validate qui permettra d'évaluer notre modèle elle prendra en paramètre : \n",
    "def validate(valid_loader, model, criterion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e1733a-4f17-4212-9a62-e6726b3ec0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_loader, model, criterion):\n",
    "    '''\n",
    "    Function for the validation step of the training loop\n",
    "    '''\n",
    "   \n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in valid_loader:\n",
    "        ###### Votre code ici ########\n",
    "        \n",
    "    running_loss /= len(valid_loader)\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb183c34-2770-42e9-bfb2-6b61a5809c2a",
   "metadata": {},
   "source": [
    "---\n",
    "<span style='color:Green'>**Question**</span>\n",
    "\n",
    "---\n",
    "Créer une fonction Train qui permettra d'entrainer notre modèle elle prendra en paramètre : \n",
    "def train(model, train_loader, optimizer, criterion, device, epochs=100): ainsi que la fonction validate précedement implémenté.\n",
    "\n",
    "--- \n",
    "**Note**\n",
    "Utiliser avant la fontion validate la fontion [.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html) pour désactiver le gradient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7446b9f-5707-4d57-ad6c-34d96b07230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device, epochs=25):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    print('Start Training')\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, (X,y_true) in enumerate(train_loader):\n",
    "            ###### Votre code ici ########\n",
    "    \n",
    "        running_loss /= len(train_loader)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss_validate = validate(valid_loader, model, criterion)\n",
    "            \n",
    "        print(f\" ecpochs: {epoch}, loss: {running_loss:.4f}, loss_validate:{running_loss_validate:.4f}\")\n",
    "            \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0ab756-8c1d-4754-a6b4-095c4e2978cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      " ecpochs: 0, loss: 0.9629, loss_validate:0.9439\n",
      " ecpochs: 1, loss: 0.9378, loss_validate:0.9294\n",
      " ecpochs: 2, loss: 0.9215, loss_validate:0.9121\n",
      " ecpochs: 3, loss: 0.9103, loss_validate:0.9040\n",
      " ecpochs: 4, loss: 0.9027, loss_validate:0.8968\n",
      " ecpochs: 5, loss: 0.8966, loss_validate:0.8918\n",
      " ecpochs: 6, loss: 0.8932, loss_validate:0.8890\n",
      " ecpochs: 7, loss: 0.8903, loss_validate:0.8863\n",
      " ecpochs: 8, loss: 0.8879, loss_validate:0.8842\n",
      " ecpochs: 9, loss: 0.8857, loss_validate:0.8822\n",
      " ecpochs: 10, loss: 0.8840, loss_validate:0.8809\n",
      " ecpochs: 11, loss: 0.8827, loss_validate:0.8798\n",
      " ecpochs: 12, loss: 0.8816, loss_validate:0.8787\n",
      " ecpochs: 13, loss: 0.8805, loss_validate:0.8777\n",
      " ecpochs: 14, loss: 0.8796, loss_validate:0.8769\n",
      " ecpochs: 15, loss: 0.8787, loss_validate:0.8760\n",
      " ecpochs: 16, loss: 0.8779, loss_validate:0.8752\n",
      " ecpochs: 17, loss: 0.8771, loss_validate:0.8746\n",
      " ecpochs: 18, loss: 0.8765, loss_validate:0.8740\n",
      " ecpochs: 19, loss: 0.8760, loss_validate:0.8735\n",
      " ecpochs: 20, loss: 0.8755, loss_validate:0.8731\n",
      " ecpochs: 21, loss: 0.8750, loss_validate:0.8728\n",
      " ecpochs: 22, loss: 0.8746, loss_validate:0.8723\n",
      " ecpochs: 23, loss: 0.8742, loss_validate:0.8720\n",
      " ecpochs: 24, loss: 0.8739, loss_validate:0.8717\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(auto_encoder,train_loader,optimizer,criterion,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c268c-22e5-4880-a06e-dfbd27f03a6a",
   "metadata": {},
   "source": [
    "**Visualiser les images produites par l'auto-encoder avec la fonction suivante:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d356556-3efe-4b13-80cc-580a6926d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "net = auto_encoder.eval()\n",
    "test_dataloader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "def show_grid(grid):\n",
    "    plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def visualize_reconstructions(net, images, device=device):\n",
    "    # Mode inférence\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        reconstructions = net(images)[0]\n",
    "        image_grid = make_grid(reconstructions[1:50], 10, 5).cpu()\n",
    "        return image_grid\n",
    "\n",
    "images, _ = next(iter(test_dataloader))\n",
    "\n",
    "# Images de test\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Images du jeu de test\")\n",
    "show_grid(make_grid(images[1:50],10,5))\n",
    "\n",
    "# Reconstruction et visualisation des images reconstruites\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Reconstruction par l'auto-encodeur\")\n",
    "show_grid(visualize_reconstructions(vae_encoder, images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef897463-b830-4821-a514-c35fc8b026e3",
   "metadata": {},
   "source": [
    "---\n",
    "<span style='color:#0d5874'> **Exercice 2 : Autoencodeurs convolutif et génération d’images** </span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25089de5-07b4-4dc8-88d1-f6bf4c2a05b3",
   "metadata": {},
   "source": [
    "Implémentation\n",
    "Notre modèle sera un auto-encodeur convolutif doté de l’architecture ci-dessous.\n",
    "\n",
    "Pour l’encodeur :\n",
    "\n",
    "* une couche de convolution (kernel_size=4, in_channels=1, out_channels=32, stride=2, padding=1, activation ReLU)\n",
    "\n",
    "* une couche de convolution (kernel_size=4, in_channels=32, out_channels=64, stride=2, padding=1, activation ReLU)\n",
    "\n",
    "* une couche linéaire (in_features=64*7*7, out_features=latent_dimension)\n",
    "\n",
    "Pour le décodeur:\n",
    "\n",
    "* une couche linéaire (in_features=latent_dimension, out_features=64*7*7, activation ReLU)\n",
    "\n",
    "* une couche de convolution transposée (kernel size=4, in_channels=64, out_channels=32, stride=2, padding=1, activation ReLU)\n",
    "\n",
    "* une couche de convolution transposée (kernel size=4, in_channels=32, out_channels=1, stride=2, padding=1, activation sigmoide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bc8b2f-8188-4bd1-bb13-1149d110e971",
   "metadata": {},
   "source": [
    "--- \n",
    "**Note**\n",
    "Les filtres convolutifs sont choisis de taille 4x4 afin d’éviter des problèmes [d’aliasing](https://distill.pub/2016/deconv-checkerboard/).\n",
    "\n",
    "<span style='color:Green'>**Question**</span>\n",
    "---\n",
    "\n",
    "Compléter l’implémentation ci-dessous de l’auto-encodeur dont l’architecture vient d’être décrite. Cette implémentation utilise l’interface torch.nn.Module dont la documentation peut vous être utile.\n",
    "\n",
    "En plus de la reconstruction par l’auto-encodeur, on souhaite que la méthode forward() renvoie également le code intermédiaire z (un vecteur de longueur latent_dimension) obtenu après le passage dans le décodeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432fb2b-70bd-4ca6-af0d-7e759b4da185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder_conv(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(AutoEncoder_conv, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.BatchNorm2d(32), \n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.BatchNorm2d(64),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Flatten(),\n",
    "                                     nn.Linear(in_features=64*7*7, out_features=latent_dimension)\n",
    "                                     )\n",
    "        self.decoder_linear = nn.Linear(in_features=latent_dimension, out_features=64*7*7)\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.BatchNorm2d(32),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.BatchNorm2d(1),\n",
    "                                     nn.Sigmoid(),\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        hat_x = F.relu(self.decoder_linear(z))\n",
    "        hat_x = hat_x.view(-1, 64, 7, 7)\n",
    "        hat_x = self.decoder(hat_x)\n",
    "        return hat_x\n",
    "    \n",
    "autoencoder_conv = AutoEncoder_conv(latent_dimension=10)\n",
    "print(autoencoder_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b71083-9343-48a2-8805-67cf23be39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder_conv.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4335f1-3f70-4502-94bc-56b661b60eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(autoencoder_conv,train_loader,optimizer,criterion,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d7011-04ed-45b3-b308-03a6e8b3e09b",
   "metadata": {},
   "source": [
    "**Visualiser les images produites par l'auto-encoder et comparer avec les images précédentes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca24cbf-88de-47f8-866d-943df6670f07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style='color:#0d5874'>  Auto-encodeurs variationnels  </span> ##\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5406f0-b41b-420f-86ef-74d7e97c89c9",
   "metadata": {},
   "source": [
    "![vae-gaussian.png](vae-gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ddb7e-1575-4220-9a86-8d8ca7b6bb09",
   "metadata": {},
   "source": [
    "Implémentation\n",
    "Nous allons à présent implémenter un VAE convolutif qui hérite de la même structure que l’auto-encodeur que nous avons précédemment défini. Pour nous simplifier les choses par la suite, nous allons commencer par séparer le sous-réseau qui définit l’encodeur de celui qui définit le décodeur.\n",
    "\n",
    "<span style='color:Green'>**Question**</span>\n",
    "\n",
    "en reprenant ce qui a été fait plus haut pour l’auto-encodeur classique, compléter les implémentations ci-dessous de l’encodeur et du décodeur pour le VAE. On rappelle que, contrairement à l’auto-encodeur, la sortie de l’encodeur est double :\n",
    "\n",
    "+ le vecteur <span style='color:Red'>mu</span> qui contient la moyenne de la gaussienne dans l’espace latent,\n",
    "+ le vecteur <span style='color:Red'>sigma</span> qui contient les variances selon les différentes directions de la gaussienne dans l’espace latent.\n",
    "\n",
    "\n",
    "le vecteur sigma qui contient les variances selon les différentes directions de la gaussienne dans l’espace latent.\n",
    "\n",
    "Ces valeurs seront les paramètres de la gaussienne associée à une observation x. Ces deux vecteurs ont pour dimension la dimension de l’espace latent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5dc1c7-0404-40b5-8db1-f6e25308bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Flatten(),\n",
    "                                   )\n",
    "        self.linear1 = nn.Linear(in_features=64*7*7, out_features=latent_dimension)\n",
    "        self.linear2 = nn.Linear(in_features=64*7*7, out_features=latent_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x_mu = self.linear1(x)\n",
    "        x_logvar = self.linear2(x)\n",
    "        return x_mu, x_logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=latent_dimension, out_features=64*7*7)\n",
    "        self.model = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "                                     nn.Sigmoid(),\n",
    "                                    )\n",
    "\n",
    "    def forward(self, z):\n",
    "        hat_x = F.relu(self.linear(z))\n",
    "        hat_x = hat_x.view(-1, 64, 7, 7)\n",
    "        hat_x = self.model(hat_x)\n",
    "        return hat_x\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953e436-4d8d-4863-a1ad-52d86cd22fd7",
   "metadata": {},
   "source": [
    "Nous allons à présent combiner l’encodeur et le décodeur pour former l’auto-encodeur variationnel complet. Il y a néanmoins une petite subtilité car nous devons implémenter l’astuce de reparamétrisation. Celle-ci est implémenter dans la méthode latent_sample\n",
    "\n",
    "Lors d’un passage avant (forward), le schéma suivant doit se dérouler :\n",
    "\n",
    "1. L’encodeur prend x en entrée et produit la moyenne mu et la variance logvar de la distribution. En pratique, pour créer mu et logvar deux couches linear en fin du décodeur.\n",
    "\n",
    "2. On tire un échantillon aléatoire z dans l’espace latent à l’aide de la méthode latent_sample. L’échantillonnage est fait selon la distribution gaussienne latente associée à x grâce à la reparamétrisation. Lors de l’inférence, on ne réalisera pas d’échantillonnage mais on se contentera d’utiliser la moyenne de la gaussienne.\n",
    "\n",
    "3. L’échantillon aléatoire z est passé dans le décodeur de sorte à obtenir la reconstruction x_recon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d7044-3aaa-40a7-bc3c-767e42bb37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encoder(x)\n",
    "        z = self.latent_sample(latent_mu, latent_logvar)\n",
    "        hat_x = self.decoder(z)\n",
    "        return hat_x, latent_mu, latent_logvar\n",
    "\n",
    "    def latent_sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # the reparameterization trick\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "vae_encoder = VariationalAutoencoder(latent_dim = 128)\n",
    "print(vae_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4323f-9ef5-4bda-a7af-0439f736635b",
   "metadata": {},
   "source": [
    "Enfin, il reste à définir la fonction de coût du VAE. D’après le cours, on cherche à maximiser $\\mathcal{L}$. Ici, on choisira de minimiser $-\\mathcal{L}$ avec\n",
    "\n",
    "\n",
    "$\\mathcal{L}(\\theta,\\phi ;\\boldsymbol{x}) =  \\underbrace{\\mathbb{E}_{q_\\phi(\\boldsymbol z | \\boldsymbol x)} \\left [ \\log p_\\theta(\\boldsymbol x | \\boldsymbol z) \\right ]}_{\\text{Esperance de la vraisemblance}} - \\underbrace{KL\\, \\left (q_\\phi(\\boldsymbol z | \\boldsymbol x) \\, || \\, p_\\theta(\\boldsymbol z)\\right)}_{\\text{ecart au prior}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c17dd3-e146-4c06-86a7-e3c51bac3e70",
   "metadata": {},
   "source": [
    "La fonction de coût pour une reconstruction sur une seule donnée $x^{(i)}$ est approximée par :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70aeb2-8562-4a02-8fec-8bbb7f7f8e90",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(\\theta,\\phi ;\\boldsymbol x^{(i)}) \\simeq - \\frac{1}{2} \\sum_j^d \\bigl ( 1 + \\log((\\sigma_j^{(i)})^2) - (\\mu_j^{(i)})^2 - (\\sigma_j^{(i)})^2  \\bigr) - \\log p_\\theta(\\boldsymbol x^{(i)} | \\boldsymbol z^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c097b8-b3d7-4829-b8f5-8260eb7971a5",
   "metadata": {},
   "source": [
    "---\n",
    "**Note**\n",
    "\n",
    "\n",
    "\n",
    "Dans la plupart des cas, la vraisemblance est supposée gaussienne et la fonction de coût évaluant la reconstruction correspondera donc à l’erreur quadratique moyenne (F.mse_loss()). Dans notre cas, la distribution des valeurs des pixels de Fashion-MNIST est plutôt bimodale. Les images étant à valeurs entre 0 et 1, il est possible d’utiliser une entropie croisée binaire (F.bce_loss()) et c’est cette version qui donne les meilleurs résultats.\n",
    "\n",
    "---\n",
    "Le prior $p_{θ}(z)$ est supposé être donné par une loi normale centrée réduite. La divergence de Kullback-Leibler est alors donnée par:\n",
    "\n",
    "$KL(q_\\phi(\\boldsymbol z | \\boldsymbol x) || p_\\theta(\\boldsymbol z)) = \\frac{1}{2} \\bigl( \\text{tr}(\\boldsymbol \\sigma \\boldsymbol I) + \\boldsymbol \\mu^T \\boldsymbol \\mu - k - \\log \\text{det}(\\boldsymbol \\sigma \\boldsymbol I)\\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb4a71-85b9-4d76-85b0-e9cbffb2ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(hat_x, x, mu, logvar):\n",
    "    reconstruction_loss = F.binary_cross_entropy(hat_x.view(-1, 28*28), x.view(-1, 28*28), reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32b4ad-ea4b-43a0-80a8-3ccf7fc24e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_loader, model, criterion):\n",
    "    '''\n",
    "    Function for the validation step of the training loop\n",
    "    '''\n",
    "   \n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in valid_loader:\n",
    "    \n",
    "        X = X.to(device)\n",
    "        \n",
    "        # Forward pass and record loss\n",
    "        X_hat, latent_mu, latent_logvar = model(X)\n",
    "        loss = vae_loss(X_hat, X, latent_mu, latent_logvar)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    running_loss /= len(valid_loader)\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768bae5-03ae-4178-ac88-4ef456fca076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, device, epochs=25):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    print('Start Training')\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, (X,y_true) in enumerate(train_loader):\n",
    "            \n",
    "            X = X.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            X_hat, latent_mu, latent_logvar = model(X)\n",
    "            loss = vae_loss(X_hat, X, latent_mu, latent_logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "        running_loss /= len(train_loader)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss_validate = validate(valid_loader, model, criterion)\n",
    "            \n",
    "        print(f\" ecpochs: {epoch}, loss: {running_loss:.4f}, loss_validate:{running_loss_validate:.4f}\")\n",
    "            \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6a43b-d379-49e4-b1ef-371331daeade",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = vae_loss\n",
    "optimizer = torch.optim.Adam(vae_encoder.parameters(),lr=0.0001)\n",
    "train(vae_encoder,train_loader,optimizer,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc39a9-757e-4165-af37-976e44db609e",
   "metadata": {},
   "source": [
    "**Visualiser les images produites par le VAE et comparer avec les images précédentes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b4ee3-e136-45ac-9062-1b31e1ac7545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
